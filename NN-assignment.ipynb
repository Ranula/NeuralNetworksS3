{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b,30.83,0,u,g,w,v,1.25,t,t,01,f,g,00202,0,+\n",
      "a,58.67,4.46,u,g,q,h,3.04,t,t,06,f,g,00043,560,+\n",
      "a,24.50,0.5,u,g,q,h,1.5,t,f,0,f,g,00280,824,+\n",
      "b,27.83,1.54,u,g,w,v,3.75,t,t,05,t,g,00100,3,+\n",
      "b,20.17,5.625,u,g,w,v,1.71,t,f,0,f,s,00120,0,+\n",
      "b,32.08,4,u,g,m,v,2.5,t,f,0,t,g,00360,0,+\n",
      "b,33.17,1.04,u,g,r,h,6.5,t,f,0,t,g,00164,31285,+\n",
      "a,22.92,11.585,u,g,cc,v,0.04,t,f,0,f,g,00080,1349,+\n",
      "b,54.42,0.5,y,p,k,h,3.96,t,f,0,f,g,00180,314,+\n",
      "b,42.50,4.915,y,p,w,v,3.165,t,f,0,t,g,00052,1442,+\n",
      "b,22.08,0.83,u,g,c,h,2.165,f,f,0,t,g,00128,0,+\n",
      "b,29.92,1.835,u,g,c,h,4.335,t,f,0,f,g,00260,200,+\n",
      "a,38.25,6,u,g,k,v,1,t,f,0,t,g,00000,0,+\n",
      "b,48.08,6.04,u,g,k,v,0.04,f,f,0,f,g,00000,2690,+\n",
      "a,45.83,10.5,u,g,q,v,5,t,t,07,t,g,00000,0,+\n",
      "b,36.67,4.415,y,p,k,v,0.25,t,t,10,t,g,00320,0,+\n",
      "b,28.25,0.875,u,g,m,v,0.96,t,t,03,t,g,00396,0,+\n",
      "a,23.25,5.875,u,g,q,v,3.17,t,t,10,f,g,00120,245,+\n",
      "b,21.83,0.25,u,g,d,h,0.665,t,f,0,t,g,00000,0,+\n",
      "a,19.17,8.585,u,g,cc,h,0.75,t,t,07,f,g,00096,0,+\n",
      "b,25.00,11.25,u,g,c,v,2.5,t,t,17,f,g,00200,1208,+\n",
      "b,23.25,1,u,g,c,v,0.835,t,f,0,f,s,00300,0,+\n",
      "a,47.75,8,u,g,c,v,7.875,t,t,06,t,g,00000,1260,+\n",
      "a,27.42,14.5,u,g,x,h,3.085,t,t,01,f,g,00120,11,+\n",
      "a,41.17,6.5,u,g,q,v,0.5,t,t,03,t,g,00145,0,+\n",
      "a,15.83,0.585,u,g,c,h,1.5,t,t,02,f,g,00100,0,+\n",
      "a,47.00,13,u,g,i,bb,5.165,t,t,09,t,g,00000,0,+\n",
      "b,56.58,18.5,u,g,d,bb,15,t,t,17,t,g,00000,0,+\n",
      "b,57.42,8.5,u,g,e,h,7,t,t,03,f,g,00000,0,+\n",
      "b,42.08,1.04,u,g,w,v,5,t,t,06,t,g,00500,10000,+\n",
      "b,29.25,14.79,u,g,aa,v,5.04,t,t,05,t,g,00168,0,+\n",
      "b,42.00,9.79,u,g,x,h,7.96,t,t,08,f,g,00000,0,+\n",
      "b,49.50,7.585,u,g,i,bb,7.585,t,t,15,t,g,00000,5000,+\n",
      "a,36.75,5.125,u,g,e,v,5,t,f,0,t,g,00000,4000,+\n",
      "a,22.58,10.75,u,g,q,v,0.415,t,t,05,t,g,00000,560,+\n",
      "b,27.83,1.5,u,g,w,v,2,t,t,11,t,g,00434,35,+\n",
      "b,27.25,1.585,u,g,cc,h,1.835,t,t,12,t,g,00583,713,+\n",
      "a,23.00,11.75,u,g,x,h,0.5,t,t,02,t,g,00300,551,+\n",
      "b,27.75,0.585,y,p,cc,v,0.25,t,t,02,f,g,00260,500,+\n",
      "b,54.58,9.415,u,g,ff,ff,14.415,t,t,11,t,g,00030,300,+\n",
      "b,34.17,9.17,u,g,c,v,4.5,t,t,12,t,g,00000,221,+\n",
      "b,28.92,15,u,g,c,h,5.335,t,t,11,f,g,00000,2283,+\n",
      "b,29.67,1.415,u,g,w,h,0.75,t,t,01,f,g,00240,100,+\n",
      "b,39.58,13.915,u,g,w,v,8.625,t,t,06,t,g,00070,0,+\n",
      "b,56.42,28,y,p,c,v,28.5,t,t,40,f,g,00000,15,+\n",
      "b,54.33,6.75,u,g,c,h,2.625,t,t,11,t,g,00000,284,+\n",
      "a,41.00,2.04,y,p,q,h,0.125,t,t,23,t,g,00455,1236,+\n",
      "b,31.92,4.46,u,g,cc,h,6.04,t,t,03,f,g,00311,300,+\n",
      "b,41.50,1.54,u,g,i,bb,3.5,f,f,0,f,g,00216,0,+\n",
      "b,23.92,0.665,u,g,c,v,0.165,f,f,0,f,g,00100,0,+\n",
      "a,25.75,0.5,u,g,c,h,0.875,t,f,0,t,g,00491,0,+\n",
      "b,26.00,1,u,g,q,v,1.75,t,f,0,t,g,00280,0,+\n",
      "b,37.42,2.04,u,g,w,v,0.04,t,f,0,t,g,00400,5800,+\n",
      "b,34.92,2.5,u,g,w,v,0,t,f,0,t,g,00239,200,+\n",
      "b,34.25,3,u,g,cc,h,7.415,t,f,0,t,g,00000,0,+\n",
      "b,23.33,11.625,y,p,w,v,0.835,t,f,0,t,g,00160,300,+\n",
      "b,23.17,0,u,g,cc,v,0.085,t,f,0,f,g,00000,0,+\n",
      "b,44.33,0.5,u,g,i,h,5,t,f,0,t,g,00320,0,+\n",
      "b,35.17,4.5,u,g,x,h,5.75,f,f,0,t,s,00711,0,+\n",
      "b,43.25,3,u,g,q,h,6,t,t,11,f,g,00080,0,+\n",
      "b,56.75,12.25,u,g,m,v,1.25,t,t,04,t,g,00200,0,+\n",
      "b,31.67,16.165,u,g,d,v,3,t,t,09,f,g,00250,730,+\n",
      "a,23.42,0.79,y,p,q,v,1.5,t,t,02,t,g,00080,400,+\n",
      "a,20.42,0.835,u,g,q,v,1.585,t,t,01,f,g,00000,0,+\n",
      "b,26.67,4.25,u,g,cc,v,4.29,t,t,01,t,g,00120,0,+\n",
      "b,34.17,1.54,u,g,cc,v,1.54,t,t,01,t,g,00520,50000,+\n",
      "a,36.00,1,u,g,c,v,2,t,t,11,f,g,00000,456,+\n",
      "b,25.50,0.375,u,g,m,v,0.25,t,t,03,f,g,00260,15108,+\n",
      "b,19.42,6.5,u,g,w,h,1.46,t,t,07,f,g,00080,2954,+\n",
      "b,35.17,25.125,u,g,x,h,1.625,t,t,01,t,g,00515,500,+\n",
      "b,32.33,7.5,u,g,e,bb,1.585,t,f,0,t,s,00420,0,-\n",
      "a,38.58,5,u,g,cc,v,13.5,t,f,0,t,g,00980,0,-\n",
      "b,44.25,0.5,u,g,m,v,10.75,t,f,0,f,s,00400,0,-\n",
      "b,44.83,7,y,p,c,v,1.625,f,f,0,f,g,00160,2,-\n",
      "b,20.67,5.29,u,g,q,v,0.375,t,t,01,f,g,00160,0,-\n",
      "b,34.08,6.5,u,g,aa,v,0.125,t,f,0,t,g,00443,0,-\n",
      "a,19.17,0.585,y,p,aa,v,0.585,t,f,0,t,g,00160,0,-\n",
      "b,21.67,1.165,y,p,k,v,2.5,t,t,01,f,g,00180,20,-\n",
      "b,21.50,9.75,u,g,c,v,0.25,t,f,0,f,g,00140,0,-\n",
      "b,49.58,19,u,g,ff,ff,0,t,t,01,f,g,00094,0,-\n",
      "a,27.67,1.5,u,g,m,v,2,t,f,0,f,s,00368,0,-\n",
      "b,39.83,0.5,u,g,m,v,0.25,t,f,0,f,s,00288,0,-\n",
      "b,27.25,0.625,u,g,aa,v,0.455,t,f,0,t,g,00200,0,-\n",
      "b,37.17,4,u,g,c,bb,5,t,f,0,t,s,00280,0,-\n",
      "b,25.67,2.21,y,p,aa,v,4,t,f,0,f,g,00188,0,-\n",
      "b,34.00,4.5,u,g,aa,v,1,t,f,0,t,g,00240,0,-\n",
      "a,49.00,1.5,u,g,j,j,0,t,f,0,t,g,00100,27,-\n",
      "b,62.50,12.75,y,p,c,h,5,t,f,0,f,g,00112,0,-\n",
      "b,31.42,15.5,u,g,c,v,0.5,t,f,0,f,g,00120,0,-\n",
      "b,52.33,1.375,y,p,c,h,9.46,t,f,0,t,g,00200,100,-\n",
      "b,28.75,1.5,y,p,c,v,1.5,t,f,0,t,g,00000,225,-\n",
      "a,28.58,3.54,u,g,i,bb,0.5,t,f,0,t,g,00171,0,-\n",
      "b,23.00,0.625,y,p,aa,v,0.125,t,f,0,f,g,00180,1,-\n",
      "a,22.50,11,y,p,q,v,3,t,f,0,t,g,00268,0,-\n",
      "a,28.50,1,u,g,q,v,1,t,t,02,t,g,00167,500,-\n",
      "b,37.50,1.75,y,p,c,bb,0.25,t,f,0,t,g,00164,400,-\n",
      "b,35.25,16.5,y,p,c,v,4,t,f,0,f,g,00080,0,-\n",
      "b,18.67,5,u,g,q,v,0.375,t,t,02,f,g,00000,38,-\n",
      "b,25.00,12,u,g,k,v,2.25,t,t,02,t,g,00120,5,-\n",
      "b,27.83,4,y,p,i,h,5.75,t,t,02,t,g,00075,0,-\n",
      "b,54.83,15.5,u,g,e,z,0,t,t,20,f,g,00152,130,-\n",
      "b,28.75,1.165,u,g,k,v,0.5,t,f,0,f,s,00280,0,-\n",
      "a,25.00,11,y,p,aa,v,4.5,t,f,0,f,g,00120,0,-\n",
      "b,40.92,2.25,y,p,x,h,10,t,f,0,t,g,00176,0,-\n",
      "a,19.75,0.75,u,g,c,v,0.795,t,t,05,t,g,00140,5,-\n",
      "b,29.17,3.5,u,g,w,v,3.5,t,t,03,t,g,00329,0,-\n",
      "a,24.50,1.04,y,p,ff,ff,0.5,t,t,03,f,g,00180,147,-\n",
      "b,24.58,12.5,u,g,w,v,0.875,t,f,0,t,g,00260,0,-\n",
      "a,33.75,0.75,u,g,k,bb,1,t,t,03,t,g,00212,0,-\n",
      "b,20.67,1.25,y,p,c,h,1.375,t,t,03,t,g,00140,210,-\n",
      "a,25.42,1.125,u,g,q,v,1.29,t,t,02,f,g,00200,0,-\n",
      "b,37.75,7,u,g,q,h,11.5,t,t,07,t,g,00300,5,-\n",
      "b,52.50,6.5,u,g,k,v,6.29,t,t,15,f,g,00000,11202,+\n",
      "b,57.83,7.04,u,g,m,v,14,t,t,06,t,g,00360,1332,+\n",
      "a,20.75,10.335,u,g,cc,h,0.335,t,t,01,t,g,00080,50,+\n",
      "b,39.92,6.21,u,g,q,v,0.04,t,t,01,f,g,00200,300,+\n",
      "b,25.67,12.5,u,g,cc,v,1.21,t,t,67,t,g,00140,258,+\n",
      "a,24.75,12.5,u,g,aa,v,1.5,t,t,12,t,g,00120,567,+\n",
      "a,44.17,6.665,u,g,q,v,7.375,t,t,03,t,g,00000,0,+\n",
      "a,23.50,9,u,g,q,v,8.5,t,t,05,t,g,00120,0,+\n",
      "b,34.92,5,u,g,x,h,7.5,t,t,06,t,g,00000,1000,+\n",
      "b,47.67,2.5,u,g,m,bb,2.5,t,t,12,t,g,00410,2510,+\n",
      "b,22.75,11,u,g,q,v,2.5,t,t,07,t,g,00100,809,+\n",
      "b,34.42,4.25,u,g,i,bb,3.25,t,t,02,f,g,00274,610,+\n",
      "a,28.42,3.5,u,g,w,v,0.835,t,f,0,f,s,00280,0,+\n",
      "b,67.75,5.5,u,g,e,z,13,t,t,01,t,g,00000,0,+\n",
      "b,20.42,1.835,u,g,c,v,2.25,t,t,01,f,g,00100,150,+\n",
      "a,47.42,8,u,g,e,bb,6.5,t,t,06,f,g,00375,51100,+\n",
      "b,36.25,5,u,g,c,bb,2.5,t,t,06,f,g,00000,367,+\n",
      "b,32.67,5.5,u,g,q,h,5.5,t,t,12,t,g,00408,1000,+\n",
      "b,48.58,6.5,u,g,q,h,6,t,f,0,t,g,00350,0,+\n",
      "b,39.92,0.54,y,p,aa,v,0.5,t,t,03,f,g,00200,1000,+\n",
      "b,33.58,2.75,u,g,m,v,4.25,t,t,06,f,g,00204,0,+\n",
      "a,18.83,9.5,u,g,w,v,1.625,t,t,06,t,g,00040,600,+\n",
      "a,26.92,13.5,u,g,q,h,5,t,t,02,f,g,00000,5000,+\n",
      "a,31.25,3.75,u,g,cc,h,0.625,t,t,09,t,g,00181,0,+\n",
      "a,56.50,16,u,g,j,ff,0,t,t,15,f,g,00000,247,+\n",
      "b,43.00,0.29,y,p,cc,h,1.75,t,t,08,f,g,00100,375,+\n",
      "b,22.33,11,u,g,w,v,2,t,t,01,f,g,00080,278,+\n",
      "b,27.25,1.665,u,g,cc,h,5.085,t,t,09,f,g,00399,827,+\n",
      "b,32.83,2.5,u,g,cc,h,2.75,t,t,06,f,g,00160,2072,+\n",
      "b,23.25,1.5,u,g,q,v,2.375,t,t,03,t,g,00000,582,+\n",
      "a,40.33,7.54,y,p,q,h,8,t,t,14,f,g,00000,2300,+\n",
      "a,30.50,6.5,u,g,c,bb,4,t,t,07,t,g,00000,3065,+\n",
      "a,52.83,15,u,g,c,v,5.5,t,t,14,f,g,00000,2200,+\n",
      "a,46.67,0.46,u,g,cc,h,0.415,t,t,11,t,g,00440,6,+\n",
      "a,58.33,10,u,g,q,v,4,t,t,14,f,g,00000,1602,+\n",
      "b,37.33,6.5,u,g,m,h,4.25,t,t,12,t,g,00093,0,+\n",
      "b,23.08,2.5,u,g,c,v,1.085,t,t,11,t,g,00060,2184,+\n",
      "b,32.75,1.5,u,g,cc,h,5.5,t,t,03,t,g,00000,0,+\n",
      "a,21.67,11.5,y,p,j,j,0,t,t,11,t,g,00000,0,+\n",
      "a,28.50,3.04,y,p,x,h,2.54,t,t,01,f,g,00070,0,+\n",
      "a,68.67,15,u,g,e,z,0,t,t,14,f,g,00000,3376,+\n",
      "b,28.00,2,u,g,k,h,4.165,t,t,02,t,g,00181,0,+\n",
      "b,34.08,0.08,y,p,m,bb,0.04,t,t,01,t,g,00280,2000,+\n",
      "b,27.67,2,u,g,x,h,1,t,t,04,f,g,00140,7544,+\n",
      "b,44.00,2,u,g,m,v,1.75,t,t,02,t,g,00000,15,+\n",
      "b,25.08,1.71,u,g,x,v,1.665,t,t,01,t,g,00395,20,+\n",
      "b,32.00,1.75,y,p,e,h,0.04,t,f,0,t,g,00393,0,+\n",
      "a,60.58,16.5,u,g,q,v,11,t,f,0,t,g,00021,10561,+\n",
      "a,40.83,10,u,g,q,h,1.75,t,f,0,f,g,00029,837,+\n",
      "b,19.33,9.5,u,g,q,v,1,t,f,0,t,g,00060,400,+\n",
      "a,32.33,0.54,u,g,cc,v,0.04,t,f,0,f,g,00440,11177,+\n",
      "b,36.67,3.25,u,g,q,h,9,t,f,0,t,g,00102,639,+\n",
      "b,37.50,1.125,y,p,d,v,1.5,f,f,0,t,g,00431,0,+\n",
      "a,25.08,2.54,y,p,aa,v,0.25,t,f,0,t,g,00370,0,+\n",
      "b,41.33,0,u,g,c,bb,15,t,f,0,f,g,00000,0,+\n",
      "b,56.00,12.5,u,g,k,h,8,t,f,0,t,g,00024,2028,+\n",
      "a,49.83,13.585,u,g,k,h,8.5,t,f,0,t,g,00000,0,+\n",
      "b,22.67,10.5,u,g,q,h,1.335,t,f,0,f,g,00100,0,+\n",
      "b,27.00,1.5,y,p,w,v,0.375,t,f,0,t,g,00260,1065,+\n",
      "b,25.00,12.5,u,g,aa,v,3,t,f,0,t,s,00020,0,+\n",
      "a,26.08,8.665,u,g,aa,v,1.415,t,f,0,f,g,00160,150,+\n",
      "a,18.42,9.25,u,g,q,v,1.21,t,t,04,f,g,00060,540,+\n",
      "b,20.17,8.17,u,g,aa,v,1.96,t,t,14,f,g,00060,158,+\n",
      "b,47.67,0.29,u,g,c,bb,15,t,t,20,f,g,00000,15000,+\n",
      "a,21.25,2.335,u,g,i,bb,0.5,t,t,04,f,s,00080,0,+\n",
      "a,20.67,3,u,g,q,v,0.165,t,t,03,f,g,00100,6,+\n",
      "a,57.08,19.5,u,g,c,v,5.5,t,t,07,f,g,00000,3000,+\n",
      "a,22.42,5.665,u,g,q,v,2.585,t,t,07,f,g,00129,3257,+\n",
      "b,48.75,8.5,u,g,c,h,12.5,t,t,09,f,g,00181,1655,+\n",
      "b,40.00,6.5,u,g,aa,bb,3.5,t,t,01,f,g,00000,500,+\n",
      "b,40.58,5,u,g,c,v,5,t,t,07,f,g,00000,3065,+\n",
      "a,28.67,1.04,u,g,c,v,2.5,t,t,05,t,g,00300,1430,+\n",
      "a,33.08,4.625,u,g,q,h,1.625,t,t,02,f,g,00000,0,+\n",
      "b,21.33,10.5,u,g,c,v,3,t,f,0,t,g,00000,0,+\n",
      "b,42.00,0.205,u,g,i,h,5.125,t,f,0,f,g,00400,0,+\n",
      "b,41.75,0.96,u,g,x,v,2.5,t,f,0,f,g,00510,600,+\n",
      "b,22.67,1.585,y,p,w,v,3.085,t,t,06,f,g,00080,0,+\n",
      "b,34.50,4.04,y,p,i,bb,8.5,t,t,07,t,g,00195,0,+\n",
      "b,28.25,5.04,y,p,c,bb,1.5,t,t,08,t,g,00144,7,+\n",
      "b,33.17,3.165,y,p,x,v,3.165,t,t,03,t,g,00380,0,+\n",
      "b,48.17,7.625,u,g,w,h,15.5,t,t,12,f,g,00000,790,+\n",
      "b,27.58,2.04,y,p,aa,v,2,t,t,03,t,g,00370,560,+\n",
      "b,22.58,10.04,u,g,x,v,0.04,t,t,09,f,g,00060,396,+\n",
      "a,24.08,0.5,u,g,q,h,1.25,t,t,01,f,g,00000,678,+\n",
      "a,41.33,1,u,g,i,bb,2.25,t,f,0,t,g,00000,300,+\n",
      "a,20.75,10.25,u,g,q,v,0.71,t,t,02,t,g,00049,0,+\n",
      "b,36.33,2.125,y,p,w,v,0.085,t,t,01,f,g,00050,1187,+\n",
      "a,35.42,12,u,g,q,h,14,t,t,08,f,g,00000,6590,+\n",
      "b,28.67,9.335,u,g,q,h,5.665,t,t,06,f,g,00381,168,+\n",
      "b,35.17,2.5,u,g,k,v,4.5,t,t,07,f,g,00150,1270,+\n",
      "b,39.50,4.25,u,g,c,bb,6.5,t,t,16,f,g,00117,1210,+\n",
      "b,39.33,5.875,u,g,cc,h,10,t,t,14,t,g,00399,0,+\n",
      "b,24.33,6.625,y,p,d,v,5.5,t,f,0,t,s,00100,0,+\n",
      "b,60.08,14.5,u,g,ff,ff,18,t,t,15,t,g,00000,1000,+\n",
      "b,23.08,11.5,u,g,i,v,3.5,t,t,09,f,g,00056,742,+\n",
      "b,26.67,2.71,y,p,cc,v,5.25,t,t,01,f,g,00211,0,+\n",
      "b,48.17,3.5,u,g,aa,v,3.5,t,f,0,f,s,00230,0,+\n",
      "b,41.17,4.04,u,g,cc,h,7,t,t,08,f,g,00320,0,+\n",
      "b,55.92,11.5,u,g,ff,ff,5,t,t,05,f,g,00000,8851,+\n",
      "b,53.92,9.625,u,g,e,v,8.665,t,t,05,f,g,00000,0,+\n",
      "a,18.92,9.25,y,p,c,v,1,t,t,04,t,g,00080,500,+\n",
      "a,50.08,12.54,u,g,aa,v,2.29,t,t,03,t,g,00156,0,+\n",
      "b,65.42,11,u,g,e,z,20,t,t,07,t,g,00022,0,+\n",
      "a,17.58,9,u,g,aa,v,1.375,t,f,0,t,g,00000,0,+\n",
      "a,18.83,9.54,u,g,aa,v,0.085,t,f,0,f,g,00100,0,+\n",
      "a,37.75,5.5,u,g,q,v,0.125,t,f,0,t,g,00228,0,+\n",
      "b,23.25,4,u,g,c,bb,0.25,t,f,0,t,g,00160,0,+\n",
      "b,18.08,5.5,u,g,k,v,0.5,t,f,0,f,g,00080,0,+\n",
      "a,22.50,8.46,y,p,x,v,2.46,f,f,0,f,g,00164,0,+\n",
      "b,19.67,0.375,u,g,q,v,2,t,t,02,t,g,00080,0,+\n",
      "b,22.08,11,u,g,cc,v,0.665,t,f,0,f,g,00100,0,+\n",
      "b,25.17,3.5,u,g,cc,v,0.625,t,t,07,f,g,00000,7059,+\n",
      "a,47.42,3,u,g,x,v,13.875,t,t,02,t,g,00519,1704,+\n",
      "b,33.50,1.75,u,g,x,h,4.5,t,t,04,t,g,00253,857,+\n",
      "b,27.67,13.75,u,g,w,v,5.75,t,f,0,t,g,00487,500,+\n",
      "a,58.42,21,u,g,i,bb,10,t,t,13,f,g,00000,6700,+\n",
      "a,20.67,1.835,u,g,q,v,2.085,t,t,05,f,g,00220,2503,+\n",
      "b,26.17,0.25,u,g,i,bb,0,t,f,0,t,g,00000,0,+\n",
      "b,21.33,7.5,u,g,aa,v,1.415,t,t,01,f,g,00080,9800,+\n",
      "b,42.83,4.625,u,g,q,v,4.58,t,f,0,f,s,00000,0,+\n",
      "b,38.17,10.125,u,g,x,v,2.5,t,t,06,f,g,00520,196,+\n",
      "b,20.50,10,y,p,c,v,2.5,t,f,0,f,s,00040,0,+\n",
      "b,48.25,25.085,u,g,w,v,1.75,t,t,03,f,g,00120,14,+\n",
      "b,28.33,5,u,g,w,v,11,t,f,0,t,g,00070,0,+\n",
      "b,18.50,2,u,g,i,v,1.5,t,t,02,f,g,00120,300,+\n",
      "b,33.17,3.04,y,p,c,h,2.04,t,t,01,t,g,00180,18027,+\n",
      "b,45.00,8.5,u,g,cc,h,14,t,t,01,t,g,00088,2000,+\n",
      "a,19.67,0.21,u,g,q,h,0.29,t,t,11,f,g,00080,99,+\n",
      "b,21.83,11,u,g,x,v,0.29,t,t,06,f,g,00121,0,+\n",
      "b,40.25,21.5,u,g,e,z,20,t,t,11,f,g,00000,1200,+\n",
      "b,41.42,5,u,g,q,h,5,t,t,06,t,g,00470,0,+\n",
      "a,17.83,11,u,g,x,h,1,t,t,11,f,g,00000,3000,+\n",
      "b,23.17,11.125,u,g,x,h,0.46,t,t,01,f,g,00100,0,+\n",
      "b,18.17,10.25,u,g,c,h,1.085,f,f,0,f,g,00320,13,-\n",
      "b,20.00,11.045,u,g,c,v,2,f,f,0,t,g,00136,0,-\n",
      "b,20.00,0,u,g,d,v,0.5,f,f,0,f,g,00144,0,-\n",
      "a,20.75,9.54,u,g,i,v,0.04,f,f,0,f,g,00200,1000,-\n",
      "a,24.50,1.75,y,p,c,v,0.165,f,f,0,f,g,00132,0,-\n",
      "b,32.75,2.335,u,g,d,h,5.75,f,f,0,t,g,00292,0,-\n",
      "a,52.17,0,y,p,ff,ff,0,f,f,0,f,g,00000,0,-\n",
      "a,48.17,1.335,u,g,i,o,0.335,f,f,0,f,g,00000,120,-\n",
      "a,20.42,10.5,y,p,x,h,0,f,f,0,t,g,00154,32,-\n",
      "b,50.75,0.585,u,g,ff,ff,0,f,f,0,f,g,00145,0,-\n",
      "b,17.08,0.085,y,p,c,v,0.04,f,f,0,f,g,00140,722,-\n",
      "b,18.33,1.21,y,p,e,dd,0,f,f,0,f,g,00100,0,-\n",
      "a,32.00,6,u,g,d,v,1.25,f,f,0,f,g,00272,0,-\n",
      "b,59.67,1.54,u,g,q,v,0.125,t,f,0,t,g,00260,0,+\n",
      "b,18.00,0.165,u,g,q,n,0.21,f,f,0,f,g,00200,40,+\n",
      "b,32.33,2.5,u,g,c,v,1.25,f,f,0,t,g,00280,0,-\n",
      "b,18.08,6.75,y,p,m,v,0.04,f,f,0,f,g,00140,0,-\n",
      "b,38.25,10.125,y,p,k,v,0.125,f,f,0,f,g,00160,0,-\n",
      "b,30.67,2.5,u,g,cc,h,2.25,f,f,0,t,s,00340,0,-\n",
      "b,18.58,5.71,u,g,d,v,0.54,f,f,0,f,g,00120,0,-\n",
      "a,19.17,5.415,u,g,i,h,0.29,f,f,0,f,g,00080,484,-\n",
      "a,18.17,10,y,p,q,h,0.165,f,f,0,f,g,00340,0,-\n",
      "b,16.25,0.835,u,g,m,v,0.085,t,f,0,f,s,00200,0,-\n",
      "b,21.17,0.875,y,p,c,h,0.25,f,f,0,f,g,00280,204,-\n",
      "b,23.92,0.585,y,p,cc,h,0.125,f,f,0,f,g,00240,1,-\n",
      "b,17.67,4.46,u,g,c,v,0.25,f,f,0,f,s,00080,0,-\n",
      "a,16.50,1.25,u,g,q,v,0.25,f,t,01,f,g,00108,98,-\n",
      "b,23.25,12.625,u,g,c,v,0.125,f,t,02,f,g,00000,5552,-\n",
      "b,17.58,10,u,g,w,h,0.165,f,t,01,f,g,00120,1,-\n",
      "b,29.50,0.58,u,g,w,v,0.29,f,t,01,f,g,00340,2803,-\n",
      "b,18.83,0.415,y,p,c,v,0.165,f,t,01,f,g,00200,1,-\n",
      "a,21.75,1.75,y,p,j,j,0,f,f,0,f,g,00160,0,-\n",
      "b,23.00,0.75,u,g,m,v,0.5,f,f,0,t,s,00320,0,-\n",
      "a,18.25,10,u,g,w,v,1,f,t,01,f,g,00120,1,-\n",
      "b,25.42,0.54,u,g,w,v,0.165,f,t,01,f,g,00272,444,-\n",
      "b,35.75,2.415,u,g,w,v,0.125,f,t,02,f,g,00220,1,-\n",
      "a,16.08,0.335,u,g,ff,ff,0,f,t,01,f,g,00160,126,-\n",
      "a,31.92,3.125,u,g,ff,ff,3.04,f,t,02,t,g,00200,4,-\n",
      "b,69.17,9,u,g,ff,ff,4,f,t,01,f,g,00070,6,-\n",
      "b,32.92,2.5,u,g,aa,v,1.75,f,t,02,t,g,00720,0,-\n",
      "b,16.33,2.75,u,g,aa,v,0.665,f,t,01,f,g,00080,21,-\n",
      "b,22.17,12.125,u,g,c,v,3.335,f,t,02,t,g,00180,173,-\n",
      "a,57.58,2,u,g,ff,ff,6.5,f,t,01,f,g,00000,10,-\n",
      "b,18.25,0.165,u,g,d,v,0.25,f,f,0,t,s,00280,0,-\n",
      "b,23.42,1,u,g,c,v,0.5,f,f,0,t,s,00280,0,-\n",
      "a,15.92,2.875,u,g,q,v,0.085,f,f,0,f,g,00120,0,-\n",
      "a,24.75,13.665,u,g,q,h,1.5,f,f,0,f,g,00280,1,-\n",
      "b,48.75,26.335,y,p,ff,ff,0,t,f,0,t,g,00000,0,-\n",
      "b,23.50,2.75,u,g,ff,ff,4.5,f,f,0,f,g,00160,25,-\n",
      "b,18.58,10.29,u,g,ff,ff,0.415,f,f,0,f,g,00080,0,-\n",
      "b,27.75,1.29,u,g,k,h,0.25,f,f,0,t,s,00140,0,-\n",
      "a,31.75,3,y,p,j,j,0,f,f,0,f,g,00160,20,-\n",
      "a,24.83,4.5,u,g,w,v,1,f,f,0,t,g,00360,6,-\n",
      "b,19.00,1.75,y,p,c,v,2.335,f,f,0,t,g,00112,6,-\n",
      "a,16.33,0.21,u,g,aa,v,0.125,f,f,0,f,g,00200,1,-\n",
      "a,18.58,10,u,g,d,v,0.415,f,f,0,f,g,00080,42,-\n",
      "b,16.25,0,y,p,aa,v,0.25,f,f,0,f,g,00060,0,-\n",
      "b,23.00,0.75,u,g,m,v,0.5,t,f,0,t,s,00320,0,-\n",
      "b,21.17,0.25,y,p,c,h,0.25,f,f,0,f,g,00280,204,-\n",
      "b,17.50,22,l,gg,ff,o,0,f,f,0,t,p,00450,100000,+\n",
      "b,19.17,0,y,p,m,bb,0,f,f,0,t,s,00500,1,+\n",
      "b,36.75,0.125,y,p,c,v,1.5,f,f,0,t,g,00232,113,+\n",
      "b,21.25,1.5,u,g,w,v,1.5,f,f,0,f,g,00150,8,+\n",
      "a,18.08,0.375,l,gg,cc,ff,10,f,f,0,t,s,00300,0,+\n",
      "a,33.67,0.375,u,g,cc,v,0.375,f,f,0,f,g,00300,44,+\n",
      "b,48.58,0.205,y,p,k,v,0.25,t,t,11,f,g,00380,2732,+\n",
      "b,33.67,1.25,u,g,w,v,1.165,f,f,0,f,g,00120,0,-\n",
      "a,29.50,1.085,y,p,x,v,1,f,f,0,f,g,00280,13,-\n",
      "b,30.17,1.085,y,p,c,v,0.04,f,f,0,f,g,00170,179,-\n",
      "b,34.83,2.5,y,p,w,v,3,f,f,0,f,s,00200,0,-\n",
      "a,33.25,2.5,y,p,c,v,2.5,f,f,0,t,g,00000,2,-\n",
      "b,34.08,2.5,u,g,c,v,1,f,f,0,f,g,00460,16,-\n",
      "a,25.25,12.5,u,g,d,v,1,f,f,0,t,g,00180,1062,-\n",
      "b,34.75,2.5,u,g,cc,bb,0.5,f,f,0,f,g,00348,0,-\n",
      "b,27.67,0.75,u,g,q,h,0.165,f,f,0,t,g,00220,251,-\n",
      "b,47.33,6.5,u,g,c,v,1,f,f,0,t,g,00000,228,-\n",
      "a,34.83,1.25,y,p,i,h,0.5,f,f,0,t,g,00160,0,-\n",
      "a,33.25,3,y,p,aa,v,2,f,f,0,f,g,00180,0,-\n",
      "b,28.00,3,u,g,w,v,0.75,f,f,0,t,g,00300,67,-\n",
      "a,39.08,4,u,g,c,v,3,f,f,0,f,g,00480,0,-\n",
      "b,42.75,4.085,u,g,aa,v,0.04,f,f,0,f,g,00108,100,-\n",
      "b,26.92,2.25,u,g,i,bb,0.5,f,f,0,t,g,00640,4000,-\n",
      "b,33.75,2.75,u,g,i,bb,0,f,f,0,f,g,00180,0,-\n",
      "b,38.92,1.75,u,g,k,v,0.5,f,f,0,t,g,00300,2,-\n",
      "b,62.75,7,u,g,e,z,0,f,f,0,f,g,00000,12,-\n",
      "b,26.75,4.5,y,p,c,bb,2.5,f,f,0,f,g,00200,1210,-\n",
      "b,63.33,0.54,u,g,c,v,0.585,t,t,03,t,g,00180,0,-\n",
      "b,27.83,1.5,u,g,w,v,2.25,f,t,01,t,g,00100,3,-\n",
      "a,26.17,2,u,g,j,j,0,f,f,0,t,g,00276,1,-\n",
      "b,22.17,0.585,y,p,ff,ff,0,f,f,0,f,g,00100,0,-\n",
      "b,22.50,11.5,y,p,m,v,1.5,f,f,0,t,g,00000,4000,-\n",
      "b,30.75,1.585,u,g,d,v,0.585,f,f,0,t,s,00000,0,-\n",
      "b,36.67,2,u,g,i,v,0.25,f,f,0,t,g,00221,0,-\n",
      "a,16.00,0.165,u,g,aa,v,1,f,t,02,t,g,00320,1,-\n",
      "b,41.17,1.335,u,g,d,v,0.165,f,f,0,f,g,00168,0,-\n",
      "a,19.50,0.165,u,g,q,v,0.04,f,f,0,t,g,00380,0,-\n",
      "b,32.42,3,u,g,d,v,0.165,f,f,0,t,g,00120,0,-\n",
      "a,36.75,4.71,u,g,ff,ff,0,f,f,0,f,g,00160,0,-\n",
      "a,30.25,5.5,u,g,k,v,5.5,f,f,0,t,s,00100,0,-\n",
      "b,23.08,2.5,u,g,ff,ff,0.085,f,f,0,t,g,00100,4208,-\n",
      "b,26.83,0.54,u,g,k,ff,0,f,f,0,f,g,00100,0,-\n",
      "b,16.92,0.335,y,p,k,v,0.29,f,f,0,f,s,00200,0,-\n",
      "b,24.42,2,u,g,e,dd,0.165,f,t,02,f,g,00320,1300,-\n",
      "b,42.83,1.25,u,g,m,v,13.875,f,t,01,t,g,00352,112,-\n",
      "a,22.75,6.165,u,g,aa,v,0.165,f,f,0,f,g,00220,1000,-\n",
      "b,39.42,1.71,y,p,m,v,0.165,f,f,0,f,s,00400,0,-\n",
      "a,23.58,11.5,y,p,k,h,3,f,f,0,t,g,00020,16,-\n",
      "b,21.42,0.75,y,p,r,n,0.75,f,f,0,t,g,00132,2,-\n",
      "b,33.00,2.5,y,p,w,v,7,f,f,0,t,g,00280,0,-\n",
      "b,26.33,13,u,g,e,dd,0,f,f,0,t,g,00140,1110,-\n",
      "a,45.00,4.585,u,g,k,h,1,f,f,0,t,s,00240,0,-\n",
      "b,26.25,1.54,u,g,w,v,0.125,f,f,0,f,g,00100,0,-\n",
      "a,20.83,0.5,y,p,e,dd,1,f,f,0,f,g,00260,0,-\n",
      "b,28.67,14.5,u,g,d,v,0.125,f,f,0,f,g,00000,286,-\n",
      "b,20.67,0.835,y,p,c,v,2,f,f,0,t,s,00240,0,-\n",
      "b,34.42,1.335,u,g,i,bb,0.125,f,f,0,t,g,00440,4500,-\n",
      "b,33.58,0.25,u,g,i,bb,4,f,f,0,t,s,00420,0,-\n",
      "b,43.17,5,u,g,i,bb,2.25,f,f,0,t,g,00141,0,-\n",
      "a,22.67,7,u,g,c,v,0.165,f,f,0,f,g,00160,0,-\n",
      "a,24.33,2.5,y,p,i,bb,4.5,f,f,0,f,g,00200,456,-\n",
      "a,56.83,4.25,y,p,ff,ff,5,f,f,0,t,g,00000,4,-\n",
      "b,22.08,11.46,u,g,k,v,1.585,f,f,0,t,g,00100,1212,-\n",
      "b,34.00,5.5,y,p,c,v,1.5,f,f,0,t,g,00060,0,-\n",
      "b,22.58,1.5,y,p,aa,v,0.54,f,f,0,t,g,00120,67,-\n",
      "b,21.17,0,u,g,c,v,0.5,f,f,0,t,s,00000,0,-\n",
      "b,26.67,14.585,u,g,i,bb,0,f,f,0,t,g,00178,0,-\n",
      "b,22.92,0.17,u,g,m,v,0.085,f,f,0,f,s,00000,0,-\n",
      "b,15.17,7,u,g,e,v,1,f,f,0,f,g,00600,0,-\n",
      "b,39.92,5,u,g,i,bb,0.21,f,f,0,f,g,00550,0,-\n",
      "b,27.42,12.5,u,g,aa,bb,0.25,f,f,0,t,g,00720,0,-\n",
      "b,24.75,0.54,u,g,m,v,1,f,f,0,t,g,00120,1,-\n",
      "b,41.17,1.25,y,p,w,v,0.25,f,f,0,f,g,00000,195,-\n",
      "a,33.08,1.625,u,g,d,v,0.54,f,f,0,t,g,00000,0,-\n",
      "b,29.83,2.04,y,p,x,h,0.04,f,f,0,f,g,00128,1,-\n",
      "a,23.58,0.585,y,p,ff,ff,0.125,f,f,0,f,g,00120,87,-\n",
      "b,26.17,12.5,y,p,k,h,1.25,f,f,0,t,g,00000,17,-\n",
      "b,31.00,2.085,u,g,c,v,0.085,f,f,0,f,g,00300,0,-\n",
      "b,20.75,5.085,y,p,j,v,0.29,f,f,0,f,g,00140,184,-\n",
      "b,28.92,0.375,u,g,c,v,0.29,f,f,0,f,g,00220,140,-\n",
      "a,51.92,6.5,u,g,i,bb,3.085,f,f,0,t,g,00073,0,-\n",
      "a,22.67,0.335,u,g,q,v,0.75,f,f,0,f,s,00160,0,-\n",
      "b,34.00,5.085,y,p,i,bb,1.085,f,f,0,t,g,00480,0,-\n",
      "a,69.50,6,u,g,ff,ff,0,f,f,0,f,s,00000,0,-\n",
      "a,19.58,0.665,y,p,c,v,1,f,t,01,f,g,02000,2,-\n",
      "b,16.00,3.125,u,g,w,v,0.085,f,t,01,f,g,00000,6,-\n",
      "b,17.08,0.25,u,g,q,v,0.335,f,t,04,f,g,00160,8,-\n",
      "b,31.25,2.835,u,g,ff,ff,0,f,t,05,f,g,00176,146,-\n",
      "b,25.17,3,u,g,c,v,1.25,f,t,01,f,g,00000,22,-\n",
      "a,22.67,0.79,u,g,i,v,0.085,f,f,0,f,g,00144,0,-\n",
      "b,40.58,1.5,u,g,i,bb,0,f,f,0,f,s,00300,0,-\n",
      "b,22.25,0.46,u,g,k,v,0.125,f,f,0,t,g,00280,55,-\n",
      "a,22.25,1.25,y,p,ff,ff,3.25,f,f,0,f,g,00280,0,-\n",
      "b,22.50,0.125,y,p,k,v,0.125,f,f,0,f,g,00200,70,-\n",
      "b,23.58,1.79,u,g,c,v,0.54,f,f,0,t,g,00136,1,-\n",
      "b,38.42,0.705,u,g,c,v,0.375,f,t,02,f,g,00225,500,-\n",
      "a,26.58,2.54,y,p,ff,ff,0,f,f,0,t,g,00180,60,-\n",
      "b,35.00,2.5,u,g,i,v,1,f,f,0,t,g,00210,0,-\n",
      "b,20.42,1.085,u,g,q,v,1.5,f,f,0,f,g,00108,7,-\n",
      "b,29.42,1.25,u,g,w,v,1.75,f,f,0,f,g,00200,0,-\n",
      "b,26.17,0.835,u,g,cc,v,1.165,f,f,0,f,g,00100,0,-\n",
      "b,33.67,2.165,u,g,c,v,1.5,f,f,0,f,p,00120,0,-\n",
      "b,24.58,1.25,u,g,c,v,0.25,f,f,0,f,g,00110,0,-\n",
      "a,27.67,2.04,u,g,w,v,0.25,f,f,0,t,g,00180,50,-\n",
      "b,37.50,0.835,u,g,e,v,0.04,f,f,0,f,g,00120,5,-\n",
      "b,49.17,2.29,u,g,ff,ff,0.29,f,f,0,f,g,00200,3,-\n",
      "b,33.58,0.335,y,p,cc,v,0.085,f,f,0,f,g,00180,0,-\n",
      "b,51.83,3,y,p,ff,ff,1.5,f,f,0,f,g,00180,4,-\n",
      "b,22.92,3.165,y,p,c,v,0.165,f,f,0,f,g,00160,1058,-\n",
      "b,21.83,1.54,u,g,k,v,0.085,f,f,0,t,g,00356,0,-\n",
      "b,25.25,1,u,g,aa,v,0.5,f,f,0,f,g,00200,0,-\n",
      "b,58.58,2.71,u,g,c,v,2.415,f,f,0,t,g,00320,0,-\n",
      "b,19.00,0,y,p,ff,ff,0,f,t,04,f,g,00045,1,-\n",
      "b,19.58,0.585,u,g,ff,ff,0,f,t,03,f,g,00350,769,-\n",
      "a,53.33,0.165,u,g,ff,ff,0,f,f,0,t,s,00062,27,-\n",
      "a,27.17,1.25,u,g,ff,ff,0,f,t,01,f,g,00092,300,-\n",
      "b,25.92,0.875,u,g,k,v,0.375,f,t,02,t,g,00174,3,-\n",
      "b,23.08,0,u,g,k,v,1,f,t,11,f,s,00000,0,-\n",
      "b,39.58,5,u,g,ff,ff,0,f,t,02,f,g,00017,1,-\n",
      "b,30.58,2.71,y,p,m,v,0.125,f,f,0,t,s,00080,0,-\n",
      "b,17.25,3,u,g,k,v,0.04,f,f,0,t,g,00160,40,-\n",
      "a,17.67,0,y,p,j,ff,0,f,f,0,f,g,00086,0,-\n",
      "b,16.50,0.125,u,g,c,v,0.165,f,f,0,f,g,00132,0,-\n",
      "a,27.33,1.665,u,g,ff,ff,0,f,f,0,f,g,00340,1,-\n",
      "b,31.25,1.125,u,g,ff,ff,0,f,t,01,f,g,00096,19,-\n",
      "b,20.00,7,u,g,c,v,0.5,f,f,0,f,g,00000,0,-\n",
      "b,39.50,1.625,u,g,c,v,1.5,f,f,0,f,g,00000,316,-\n",
      "b,36.50,4.25,u,g,q,v,3.5,f,f,0,f,g,00454,50,-\n",
      "b,52.42,1.5,u,g,d,v,3.75,f,f,0,t,g,00000,350,-\n",
      "b,36.17,18.125,u,g,w,v,0.085,f,f,0,f,g,00320,3552,-\n",
      "b,29.67,0.75,y,p,c,v,0.04,f,f,0,f,g,00240,0,-\n",
      "b,36.17,5.5,u,g,i,bb,5,f,f,0,f,g,00210,687,-\n",
      "b,25.67,0.29,y,p,c,v,1.5,f,f,0,t,g,00160,0,-\n",
      "a,24.50,2.415,y,p,c,v,0,f,f,0,f,g,00120,0,-\n",
      "b,24.08,0.875,u,g,m,v,0.085,f,t,04,f,g,00254,1950,-\n",
      "b,21.92,0.5,u,g,c,v,0.125,f,f,0,f,g,00360,0,-\n",
      "a,36.58,0.29,u,g,ff,ff,0,f,t,10,f,g,00200,18,-\n",
      "a,23.00,1.835,u,g,j,j,0,f,t,01,f,g,00200,53,-\n",
      "a,27.58,3,u,g,m,v,2.79,f,t,01,t,g,00280,10,-\n",
      "b,31.08,3.085,u,g,c,v,2.5,f,t,02,t,g,00160,41,-\n",
      "a,30.42,1.375,u,g,w,h,0.04,f,t,03,f,g,00000,33,-\n",
      "b,22.08,2.335,u,g,k,v,0.75,f,f,0,f,g,00180,0,-\n",
      "b,16.33,4.085,u,g,i,h,0.415,f,f,0,t,g,00120,0,-\n",
      "a,21.92,11.665,u,g,k,h,0.085,f,f,0,f,g,00320,5,-\n",
      "b,21.08,4.125,y,p,i,h,0.04,f,f,0,f,g,00140,100,-\n",
      "b,17.42,6.5,u,g,i,v,0.125,f,f,0,f,g,00060,100,-\n",
      "b,19.17,4,y,p,i,v,1,f,f,0,t,g,00360,1000,-\n",
      "b,20.67,0.415,u,g,c,v,0.125,f,f,0,f,g,00000,44,-\n",
      "b,26.75,2,u,g,d,v,0.75,f,f,0,t,g,00080,0,-\n",
      "b,23.58,0.835,u,g,i,h,0.085,f,f,0,t,g,00220,5,-\n",
      "b,39.17,2.5,y,p,i,h,10,f,f,0,t,s,00200,0,-\n",
      "b,22.75,11.5,u,g,i,v,0.415,f,f,0,f,g,00000,0,-\n",
      "a,16.92,0.5,u,g,i,v,0.165,f,t,06,t,g,00240,35,-\n",
      "b,23.50,3.165,y,p,k,v,0.415,f,t,01,t,g,00280,80,-\n",
      "a,17.33,9.5,u,g,aa,v,1.75,f,t,10,t,g,00000,10,-\n",
      "b,23.75,0.415,y,p,c,v,0.04,f,t,02,f,g,00128,6,-\n",
      "b,34.67,1.08,u,g,m,v,1.165,f,f,0,f,s,00028,0,-\n",
      "b,74.83,19,y,p,ff,ff,0.04,f,t,02,f,g,00000,351,-\n",
      "b,28.17,0.125,y,p,k,v,0.085,f,f,0,f,g,00216,2100,-\n",
      "b,24.50,13.335,y,p,aa,v,0.04,f,f,0,t,g,00120,475,-\n",
      "b,18.83,3.54,y,p,ff,ff,0,f,f,0,t,g,00180,1,-\n",
      "a,47.25,0.75,u,g,q,h,2.75,t,t,01,f,g,00333,892,+\n",
      "b,24.17,0.875,u,g,q,v,4.625,t,t,02,t,g,00520,2000,+\n",
      "b,39.25,9.5,u,g,m,v,6.5,t,t,14,f,g,00240,4607,+\n",
      "a,20.50,11.835,u,g,c,h,6,t,f,0,f,g,00340,0,+\n",
      "a,18.83,4.415,y,p,c,h,3,t,f,0,f,g,00240,0,+\n",
      "b,19.17,9.5,u,g,w,v,1.5,t,f,0,f,g,00120,2206,+\n",
      "a,25.00,0.875,u,g,x,h,1.04,t,f,0,t,g,00160,5860,+\n",
      "b,20.17,9.25,u,g,c,v,1.665,t,t,03,t,g,00040,28,+\n",
      "b,25.75,0.5,u,g,c,v,1.46,t,t,05,t,g,00312,0,+\n",
      "b,20.42,7,u,g,c,v,1.625,t,t,03,f,g,00200,1391,+\n",
      "b,39.00,5,u,g,cc,v,3.5,t,t,10,t,g,00000,0,+\n",
      "a,64.08,0.165,u,g,ff,ff,0,t,t,01,f,g,00232,100,+\n",
      "b,28.25,5.125,u,g,x,v,4.75,t,t,02,f,g,00420,7,+\n",
      "a,28.75,3.75,u,g,c,v,1.085,t,t,01,t,g,00371,0,+\n",
      "b,31.33,19.5,u,g,c,v,7,t,t,16,f,g,00000,5000,+\n",
      "a,18.92,9,u,g,aa,v,0.75,t,t,02,f,g,00088,591,+\n",
      "a,24.75,3,u,g,q,h,1.835,t,t,19,f,g,00000,500,+\n",
      "a,30.67,12,u,g,c,v,2,t,t,01,f,g,00220,19,+\n",
      "b,21.00,4.79,y,p,w,v,2.25,t,t,01,t,g,00080,300,+\n",
      "b,13.75,4,y,p,w,v,1.75,t,t,02,t,g,00120,1000,+\n",
      "a,46.00,4,u,g,j,j,0,t,f,0,f,g,00100,960,+\n",
      "a,44.33,0,u,g,c,v,2.5,t,f,0,f,g,00000,0,+\n",
      "b,20.25,9.96,u,g,e,dd,0,t,f,0,f,g,00000,0,+\n",
      "b,22.67,2.54,y,p,c,h,2.585,t,f,0,f,g,00000,0,+\n",
      "a,60.92,5,u,g,aa,v,4,t,t,04,f,g,00000,99,+\n",
      "b,16.08,0.75,u,g,c,v,1.75,t,t,05,t,g,00352,690,+\n",
      "a,28.17,0.375,u,g,q,v,0.585,t,t,04,f,g,00080,0,+\n",
      "b,39.17,1.71,u,g,x,v,0.125,t,t,05,t,g,00480,0,+\n",
      "a,30.00,5.29,u,g,e,dd,2.25,t,t,05,t,g,00099,500,+\n",
      "b,22.83,3,u,g,m,v,1.29,t,t,01,f,g,00260,800,+\n",
      "a,22.50,8.5,u,g,q,v,1.75,t,t,10,f,g,00080,990,-\n",
      "a,28.58,1.665,u,g,q,v,2.415,t,f,0,t,g,00440,0,-\n",
      "b,45.17,1.5,u,g,c,v,2.5,t,f,0,t,g,00140,0,-\n",
      "b,41.58,1.75,u,g,k,v,0.21,t,f,0,f,g,00160,0,-\n",
      "a,57.08,0.335,u,g,i,bb,1,t,f,0,t,g,00252,2197,-\n",
      "a,55.75,7.08,u,g,k,h,6.75,t,t,03,t,g,00100,50,-\n",
      "b,43.25,25.21,u,g,q,h,0.21,t,t,01,f,g,00760,90,-\n",
      "a,25.33,2.085,u,g,c,h,2.75,t,f,0,t,g,00360,1,-\n",
      "a,24.58,0.67,u,g,aa,h,1.75,t,f,0,f,g,00400,0,-\n",
      "b,43.17,2.25,u,g,i,bb,0.75,t,f,0,f,g,00560,0,-\n",
      "b,40.92,0.835,u,g,ff,ff,0,t,f,0,f,g,00130,1,-\n",
      "b,31.83,2.5,u,g,aa,v,7.5,t,f,0,t,g,00523,0,-\n",
      "a,33.92,1.585,y,p,ff,ff,0,t,f,0,f,g,00320,0,-\n",
      "a,24.92,1.25,u,g,ff,ff,0,t,f,0,f,g,00080,0,-\n",
      "b,35.25,3.165,u,g,x,h,3.75,t,f,0,t,g,00680,0,-\n",
      "b,34.25,1.75,u,g,w,bb,0.25,t,f,0,t,g,00163,0,-\n",
      "b,19.42,1.5,y,p,cc,v,2,t,f,0,t,g,00100,20,-\n",
      "b,42.75,3,u,g,i,bb,1,t,f,0,f,g,00000,200,-\n",
      "b,19.67,10,y,p,k,h,0.835,t,f,0,t,g,00140,0,-\n",
      "b,36.33,3.79,u,g,w,v,1.165,t,f,0,t,g,00200,0,-\n",
      "b,30.08,1.04,y,p,i,bb,0.5,t,t,10,t,g,00132,28,-\n",
      "b,44.25,11,y,p,d,v,1.5,t,f,0,f,s,00000,0,-\n",
      "b,23.58,0.46,y,p,w,v,2.625,t,t,06,t,g,00208,347,-\n",
      "b,23.92,1.5,u,g,d,h,1.875,t,t,06,f,g,00200,327,+\n",
      "b,33.17,1,u,g,x,v,0.75,t,t,07,t,g,00340,4071,+\n",
      "b,48.33,12,u,g,m,v,16,t,f,0,f,s,00110,0,+\n",
      "b,76.75,22.29,u,g,e,z,12.75,t,t,01,t,g,00000,109,+\n",
      "b,51.33,10,u,g,i,bb,0,t,t,11,f,g,00000,1249,+\n",
      "b,34.75,15,u,g,r,n,5.375,t,t,09,t,g,00000,134,+\n",
      "b,38.58,3.335,u,g,w,v,4,t,t,14,f,g,00383,1344,+\n",
      "a,22.42,11.25,y,p,x,h,0.75,t,t,04,f,g,00000,321,+\n",
      "b,41.92,0.42,u,g,c,h,0.21,t,t,06,f,g,00220,948,+\n",
      "b,29.58,4.5,u,g,w,v,7.5,t,t,02,t,g,00330,0,+\n",
      "a,32.17,1.46,u,g,w,v,1.085,t,t,16,f,g,00120,2079,+\n",
      "b,51.42,0.04,u,g,x,h,0.04,t,f,0,f,g,00000,3000,+\n",
      "a,22.83,2.29,u,g,q,h,2.29,t,t,07,t,g,00140,2384,+\n",
      "a,25.00,12.33,u,g,cc,h,3.5,t,t,06,f,g,00400,458,+\n",
      "b,26.75,1.125,u,g,x,h,1.25,t,f,0,f,g,00000,5298,+\n",
      "b,23.33,1.5,u,g,c,h,1.415,t,f,0,f,g,00422,200,+\n",
      "b,24.42,12.335,u,g,q,h,1.585,t,f,0,t,g,00120,0,+\n",
      "b,42.17,5.04,u,g,q,h,12.75,t,f,0,t,g,00092,0,+\n",
      "a,20.83,3,u,g,aa,v,0.04,t,f,0,f,g,00100,0,+\n",
      "b,23.08,11.5,u,g,w,h,2.125,t,t,11,t,g,00290,284,+\n",
      "a,25.17,2.875,u,g,x,h,0.875,t,f,0,f,g,00360,0,+\n",
      "b,43.08,0.375,y,p,c,v,0.375,t,t,08,t,g,00300,162,+\n",
      "a,35.75,0.915,u,g,aa,v,0.75,t,t,04,f,g,00000,1583,+\n",
      "b,59.50,2.75,u,g,w,v,1.75,t,t,05,t,g,00060,58,+\n",
      "b,21.00,3,y,p,d,v,1.085,t,t,08,t,g,00160,1,+\n",
      "b,21.92,0.54,y,p,x,v,0.04,t,t,01,t,g,00840,59,+\n",
      "a,65.17,14,u,g,ff,ff,0,t,t,11,t,g,00000,1400,+\n",
      "a,20.33,10,u,g,c,h,1,t,t,04,f,g,00050,1465,+\n",
      "b,32.25,0.165,y,p,c,h,3.25,t,t,01,t,g,00432,8000,+\n",
      "b,30.17,0.5,u,g,c,v,1.75,t,t,11,f,g,00032,540,+\n",
      "b,25.17,6,u,g,c,v,1,t,t,03,f,g,00000,0,+\n",
      "b,39.17,1.625,u,g,c,v,1.5,t,t,10,f,g,00186,4700,+\n",
      "b,39.08,6,u,g,m,v,1.29,t,t,05,t,g,00108,1097,+\n",
      "b,31.67,0.83,u,g,x,v,1.335,t,t,08,t,g,00303,3290,+\n",
      "b,41.00,0.04,u,g,e,v,0.04,f,t,01,f,s,00560,0,+\n",
      "b,48.50,4.25,u,g,m,v,0.125,t,f,0,t,g,00225,0,+\n",
      "b,32.67,9,y,p,w,h,5.25,t,f,0,t,g,00154,0,+\n",
      "a,28.08,15,y,p,e,z,0,t,f,0,f,g,00000,13212,+\n",
      "b,73.42,17.75,u,g,ff,ff,0,t,f,0,t,g,00000,0,+\n",
      "b,64.08,20,u,g,x,h,17.5,t,t,09,t,g,00000,1000,+\n",
      "b,51.58,15,u,g,c,v,8.5,t,t,09,f,g,00000,0,+\n",
      "b,26.67,1.75,y,p,c,v,1,t,t,05,t,g,00160,5777,+\n",
      "b,25.33,0.58,u,g,c,v,0.29,t,t,07,t,g,00096,5124,+\n",
      "b,30.17,6.5,u,g,cc,v,3.125,t,t,08,f,g,00330,1200,+\n",
      "b,27.00,0.75,u,g,c,h,4.25,t,t,03,t,g,00312,150,+\n",
      "b,34.17,5.25,u,g,w,v,0.085,f,f,0,t,g,00290,6,+\n",
      "b,38.67,0.21,u,g,k,v,0.085,t,f,0,t,g,00280,0,+\n",
      "b,25.75,0.75,u,g,c,bb,0.25,t,f,0,f,g,00349,23,+\n",
      "a,46.08,3,u,g,c,v,2.375,t,t,08,t,g,00396,4159,+\n",
      "a,21.50,6,u,g,aa,v,2.5,t,t,03,f,g,00080,918,+\n",
      "b,20.50,2.415,u,g,c,v,2,t,t,11,t,g,00200,3000,+\n",
      "a,29.50,0.46,u,g,k,v,0.54,t,t,04,f,g,00380,500,+\n",
      "b,29.83,1.25,y,p,k,v,0.25,f,f,0,f,g,00224,0,-\n",
      "b,20.08,0.25,u,g,q,v,0.125,f,f,0,f,g,00200,0,-\n",
      "b,23.42,0.585,u,g,c,h,0.085,t,f,0,f,g,00180,0,-\n",
      "a,29.58,1.75,y,p,k,v,1.25,f,f,0,t,g,00280,0,-\n",
      "b,16.17,0.04,u,g,c,v,0.04,f,f,0,f,g,00000,0,+\n",
      "b,32.33,3.5,u,g,k,v,0.5,f,f,0,t,g,00232,0,-\n",
      "b,47.83,4.165,u,g,x,bb,0.085,f,f,0,t,g,00520,0,-\n",
      "b,20.00,1.25,y,p,k,v,0.125,f,f,0,f,g,00140,4,-\n",
      "b,27.58,3.25,y,p,q,h,5.085,f,t,02,t,g,00369,1,-\n",
      "b,22.00,0.79,u,g,w,v,0.29,f,t,01,f,g,00420,283,-\n",
      "b,19.33,10.915,u,g,c,bb,0.585,f,t,02,t,g,00200,7,-\n",
      "a,38.33,4.415,u,g,c,v,0.125,f,f,0,f,g,00160,0,-\n",
      "b,29.42,1.25,u,g,c,h,0.25,f,t,02,t,g,00400,108,-\n",
      "b,22.67,0.75,u,g,i,v,1.585,f,t,01,t,g,00400,9,-\n",
      "b,32.25,14,y,p,ff,ff,0,f,t,02,f,g,00160,1,-\n",
      "b,29.58,4.75,u,g,m,v,2,f,t,01,t,g,00460,68,-\n",
      "b,18.42,10.415,y,p,aa,v,0.125,t,f,0,f,g,00120,375,-\n",
      "b,22.17,2.25,u,g,i,v,0.125,f,f,0,f,g,00160,10,-\n",
      "b,22.67,0.165,u,g,c,j,2.25,f,f,0,t,s,00000,0,+\n",
      "b,18.83,0,u,g,q,v,0.665,f,f,0,f,g,00160,1,-\n",
      "b,21.58,0.79,y,p,cc,v,0.665,f,f,0,f,g,00160,0,-\n",
      "b,23.75,12,u,g,c,v,2.085,f,f,0,f,s,00080,0,-\n",
      "b,36.08,2.54,u,g,ff,ff,0,f,f,0,f,g,00000,1000,-\n",
      "b,29.25,13,u,g,d,h,0.5,f,f,0,f,g,00228,0,-\n",
      "a,19.58,0.665,u,g,w,v,1.665,f,f,0,f,g,00220,5,-\n",
      "a,22.92,1.25,u,g,q,v,0.25,f,f,0,t,g,00120,809,-\n",
      "a,27.25,0.29,u,g,m,h,0.125,f,t,01,t,g,00272,108,-\n",
      "a,38.75,1.5,u,g,ff,ff,0,f,f,0,f,g,00076,0,-\n",
      "b,32.42,2.165,y,p,k,ff,0,f,f,0,f,g,00120,0,-\n",
      "a,23.75,0.71,u,g,w,v,0.25,f,t,01,t,g,00240,4,-\n",
      "b,18.17,2.46,u,g,c,n,0.96,f,t,02,t,g,00160,587,-\n",
      "b,40.92,0.5,y,p,m,v,0.5,f,f,0,t,g,00130,0,-\n",
      "b,19.50,9.585,u,g,aa,v,0.79,f,f,0,f,g,00080,350,-\n",
      "b,28.58,3.625,u,g,aa,v,0.25,f,f,0,t,g,00100,0,-\n",
      "b,35.58,0.75,u,g,k,v,1.5,f,f,0,t,g,00231,0,-\n",
      "b,34.17,2.75,u,g,i,bb,2.5,f,f,0,t,g,00232,200,-\n",
      "b,31.58,0.75,y,p,aa,v,3.5,f,f,0,t,g,00320,0,-\n",
      "a,52.50,7,u,g,aa,h,3,f,f,0,f,g,00000,0,-\n",
      "b,36.17,0.42,y,p,w,v,0.29,f,f,0,t,g,00309,2,-\n",
      "b,37.33,2.665,u,g,cc,v,0.165,f,f,0,t,g,00000,501,-\n",
      "a,20.83,8.5,u,g,c,v,0.165,f,f,0,f,g,00000,351,-\n",
      "b,24.08,9,u,g,aa,v,0.25,f,f,0,t,g,00000,0,-\n",
      "b,25.58,0.335,u,g,k,h,3.5,f,f,0,t,g,00340,0,-\n",
      "a,35.17,3.75,u,g,ff,ff,0,f,t,06,f,g,00000,200,-\n",
      "b,48.08,3.75,u,g,i,bb,1,f,f,0,f,g,00100,2,-\n",
      "a,15.83,7.625,u,g,q,v,0.125,f,t,01,t,g,00000,160,-\n",
      "a,22.50,0.415,u,g,i,v,0.335,f,f,0,t,s,00144,0,-\n",
      "b,21.50,11.5,u,g,i,v,0.5,t,f,0,t,g,00100,68,-\n",
      "a,23.58,0.83,u,g,q,v,0.415,f,t,01,t,g,00200,11,-\n",
      "a,21.08,5,y,p,ff,ff,0,f,f,0,f,g,00000,0,-\n",
      "b,25.67,3.25,u,g,c,h,2.29,f,t,01,t,g,00416,21,-\n",
      "a,38.92,1.665,u,g,aa,v,0.25,f,f,0,f,g,00000,390,-\n",
      "a,15.75,0.375,u,g,c,v,1,f,f,0,f,g,00120,18,-\n",
      "a,28.58,3.75,u,g,c,v,0.25,f,t,01,t,g,00040,154,-\n",
      "b,22.25,9,u,g,aa,v,0.085,f,f,0,f,g,00000,0,-\n",
      "b,29.83,3.5,u,g,c,v,0.165,f,f,0,f,g,00216,0,-\n",
      "a,23.50,1.5,u,g,w,v,0.875,f,f,0,t,g,00160,0,-\n",
      "b,32.08,4,y,p,cc,v,1.5,f,f,0,t,g,00120,0,-\n",
      "b,31.08,1.5,y,p,w,v,0.04,f,f,0,f,s,00160,0,-\n",
      "b,31.83,0.04,y,p,m,v,0.04,f,f,0,f,g,00000,0,-\n",
      "a,21.75,11.75,u,g,c,v,0.25,f,f,0,t,g,00180,0,-\n",
      "a,17.92,0.54,u,g,c,v,1.75,f,t,01,t,g,00080,5,-\n",
      "b,30.33,0.5,u,g,d,h,0.085,f,f,0,t,s,00252,0,-\n",
      "b,51.83,2.04,y,p,ff,ff,1.5,f,f,0,f,g,00120,1,-\n",
      "b,47.17,5.835,u,g,w,v,5.5,f,f,0,f,g,00465,150,-\n",
      "b,25.83,12.835,u,g,cc,v,0.5,f,f,0,f,g,00000,2,-\n",
      "a,50.25,0.835,u,g,aa,v,0.5,f,f,0,t,g,00240,117,-\n",
      "a,37.33,2.5,u,g,i,h,0.21,f,f,0,f,g,00260,246,-\n",
      "a,41.58,1.04,u,g,aa,v,0.665,f,f,0,f,g,00240,237,-\n",
      "a,30.58,10.665,u,g,q,h,0.085,f,t,12,t,g,00129,3,-\n",
      "b,19.42,7.25,u,g,m,v,0.04,f,t,01,f,g,00100,1,-\n",
      "a,17.92,10.21,u,g,ff,ff,0,f,f,0,f,g,00000,50,-\n",
      "a,20.08,1.25,u,g,c,v,0,f,f,0,f,g,00000,0,-\n",
      "b,19.50,0.29,u,g,k,v,0.29,f,f,0,f,g,00280,364,-\n",
      "b,27.83,1,y,p,d,h,3,f,f,0,f,g,00176,537,-\n",
      "b,17.08,3.29,u,g,i,v,0.335,f,f,0,t,g,00140,2,-\n",
      "b,36.42,0.75,y,p,d,v,0.585,f,f,0,f,g,00240,3,-\n",
      "b,40.58,3.29,u,g,m,v,3.5,f,f,0,t,s,00400,0,-\n",
      "b,21.08,10.085,y,p,e,h,1.25,f,f,0,f,g,00260,0,-\n",
      "a,22.67,0.75,u,g,c,v,2,f,t,02,t,g,00200,394,-\n",
      "a,25.25,13.5,y,p,ff,ff,2,f,t,01,t,g,00200,1,-\n",
      "b,17.92,0.205,u,g,aa,v,0.04,f,f,0,f,g,00280,750,-\n",
      "b,35.00,3.375,u,g,c,h,8.29,f,f,0,t,g,00000,0,-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "cleaned_data = \"\"\n",
    "cleaned_p_count = 0\n",
    "cleaned_n_count = 0\n",
    "\n",
    "with open('dataset/crx.data', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    for i, row in enumerate(data):\n",
    "        # Check for '?' value in each row (indicates missing)\n",
    "        if '?' not in row:\n",
    "            cleaned_data += row\n",
    "            if '+' in row:\n",
    "                cleaned_p_count += 1\n",
    "            elif '-' in row:\n",
    "                cleaned_n_count += 1\n",
    "\n",
    "    print(cleaned_data)\n",
    "\n",
    "with open('./dataset/crx_clean.data.txt', 'w') as f:\n",
    "    f.write(cleaned_data)\n",
    "\n",
    "with open('./dataset/crx_clean.names.txt', 'w') as f:\n",
    "    f.write(\"Class Distribution\\n\")\n",
    "    f.write(\"+ Classes: %d\\n\" %cleaned_p_count)\n",
    "    f.write(\"- Classes: %d\\n\" %cleaned_n_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_category(credit_data):\n",
    "\t\"\"\"\n",
    "\tSplits 'category' columns into one-hot columns\n",
    "\targ, return\n",
    "\t\tcredit_data: Dataframe\n",
    "\t\"\"\"\n",
    "\tcat_columns = []\n",
    "\tfor i, _ in enumerate(credit_data):\n",
    "\t\t# dtype == 'object' after ensuring data has been cleaned\n",
    "\t\t# i.e no 'float' dtypes as 'object' because of '?' values\n",
    "\t\tif credit_data[i].dtype == 'object' and not i==15:\n",
    "\t\t\tcat_columns.append(i)\n",
    "\n",
    "\n",
    "\t# get_dummies() one-hot encodes data\n",
    "\tcredit_data = pd.get_dummies(credit_data, columns=cat_columns)\n",
    "\t\n",
    "\treturn credit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "\n",
    "def import_data(url):\n",
    "\t\"\"\"\n",
    "\targs\n",
    "\t\turl: url string of CLEANED csv data\n",
    "\treturns\n",
    "\t\tcredit_data: Dataframe\n",
    "\t\"\"\"\n",
    "\n",
    "\tcredit_data = pd.read_csv(url, sep=',', header=None)\n",
    "\n",
    "\t# Bring class attribute to first column\n",
    "\tcols = credit_data.columns.tolist()\n",
    "\tcols = cols[-1:] + cols[:-1]\n",
    "\tcredit_data = credit_data[cols]\n",
    "\tprint(\"Reordered Dataset: \\n\", credit_data.head())\n",
    "\n",
    "\tcredit_data = one_hot_encode_category(credit_data)\n",
    "\tprint(\"Dataset length: \", len(credit_data))\n",
    "\tprint(\"Dataset shape: \", credit_data.shape)\n",
    "\tprint(\"One-hot Dataset: \\n\", credit_data.head())\n",
    "\t# print(credit_data.info())\n",
    "\treturn credit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered Dataset: \n",
      "   15 0      1      2  3  4  5  6     7  8  9   10 11 12   13   14\n",
      "0  +  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  202    0\n",
      "1  +  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g   43  560\n",
      "2  +  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  280  824\n",
      "3  +  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  100    3\n",
      "4  +  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  120    0\n",
      "Dataset length:  653\n",
      "Dataset shape:  (653, 47)\n",
      "One-hot Dataset: \n",
      "   15      1      2     7  10   13   14  0_a  0_b  3_l  ...  6_z  8_f  8_t  \\\n",
      "0  +  30.83  0.000  1.25   1  202    0    0    1    0  ...    0    0    1   \n",
      "1  +  58.67  4.460  3.04   6   43  560    1    0    0  ...    0    0    1   \n",
      "2  +  24.50  0.500  1.50   0  280  824    1    0    0  ...    0    0    1   \n",
      "3  +  27.83  1.540  3.75   5  100    3    0    1    0  ...    0    0    1   \n",
      "4  +  20.17  5.625  1.71   0  120    0    0    1    0  ...    0    0    1   \n",
      "\n",
      "   9_f  9_t  11_f  11_t  12_g  12_p  12_s  \n",
      "0    0    1     1     0     1     0     0  \n",
      "1    0    1     1     0     1     0     0  \n",
      "2    1    0     1     0     1     0     0  \n",
      "3    0    1     0     1     1     0     0  \n",
      "4    1    0     1     0     0     0     1  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "#Read filtered data and feature setup and split to feed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Building Phase\n",
    "data = import_data(\"dataset/crx_clean.data.txt\")\n",
    "\n",
    "X = data.values[:, 1:]\n",
    "Y = data.values[:, 0]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = seed)\n",
    "\n",
    "#Float conversion\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "\n",
    "Y_train = np.where(Y_train=='+', 1, Y_train)\n",
    "Y_train = np.where(Y_train=='-', 0, Y_train)\n",
    "Y_train = Y_train.astype(np.float)\n",
    "\n",
    "Y_test = np.where(Y_test=='+', 1, Y_test)\n",
    "Y_test = np.where(Y_test=='-', 0, Y_test)\n",
    "Y_test = Y_test.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((Y_train, Y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(optimizerF, lossF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(optimizerF, lossF):\n",
    "    print(f'**************************************{optimizerF}----{lossF}**************************************')\n",
    "    # Define per-fold score containers\n",
    "    f1_per_fold = []\n",
    "    loss_per_fold = []\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "    # Define the model architecture\n",
    "        model = get_model(optimizerF, lossF)\n",
    "#         print('-------------------------------------------------------------------------------------------------')\n",
    "#         print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "        history = model.fit(inputs[train], targets[train],batch_size=10,epochs=50, verbose=0)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "#         print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]} {model.metrics_names[1]} of {scores[1]}')\n",
    "        f1_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "    # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score per fold')\n",
    "    for i in range(0, len(f1_per_fold)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - F1 score: {f1_per_fold[i]}')\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('F1 scores for all folds:')\n",
    "    print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "    print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "    print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimzers = ['adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "lossFunctions = ['binary_crossentropy', 'mean_squared_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************adam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.506957471370697 - F1 score: 0.7749541997909546\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.43312200903892517 - F1 score: 0.7996380925178528\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6460927128791809 - F1 score: 0.8259073495864868\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.41890308260917664 - F1 score: 0.874173641204834\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.9617670774459839 - F1 score: 0.7384443283081055\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.8026235222816467 (+- 0.04594568373404788)\n",
      "> Loss: 0.5933684706687927\n",
      "------------------------------------------------------------------------\n",
      "**************************************adam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3133220970630646 - F1 score: 0.41225433349609375\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.3522473871707916 - F1 score: 0.33982545137405396\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.37824153900146484 - F1 score: 0.5160172581672668\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.14554071426391602 - F1 score: 0.6570618152618408\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.35185280442237854 - F1 score: 0.584886908531189\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5020091533660889 (+- 0.11442531026308986)\n",
      "> Loss: 0.3082409083843231\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6985694766044617 - F1 score: 0.299393892288208\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6836435794830322 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6666374206542969 - F1 score: 0.2031954824924469\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6723915338516235 - F1 score: 0.08916407078504562\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.7074745893478394 - F1 score: 0.13543859124183655\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.14543840736150743 (+- 0.1014468320669467)\n",
      "> Loss: 0.6857433199882508\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.43984583020210266 - F1 score: 0.10944442451000214\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.22809889912605286 - F1 score: 0.5785073041915894\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3498094081878662 - F1 score: 0.13725487887859344\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.20204044878482819 - F1 score: 0.5262213349342346\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.44202402234077454 - F1 score: 0.09297384321689606\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.28888035714626314 (+- 0.2162320096438306)\n",
      "> Loss: 0.3323637217283249\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.36587536334991455 - F1 score: 0.846063494682312\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.1477614641189575 - F1 score: 0.5921720266342163\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6572077870368958 - F1 score: 0.7772160768508911\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3362944424152374 - F1 score: 0.8893033862113953\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.8259541392326355 - F1 score: 0.8070090413093567\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7823528051376343 (+- 0.1022746529948522)\n",
      "> Loss: 0.6666186392307282\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.326553612947464 - F1 score: 0.5423340201377869\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.18044854700565338 - F1 score: 0.7587984204292297\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.16233758628368378 - F1 score: 0.6299451589584351\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.15316985547542572 - F1 score: 0.7996373176574707\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.1080251932144165 - F1 score: 0.888847827911377\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7239125490188598 (+- 0.12325221570273459)\n",
      "> Loss: 0.18610695898532867\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 27.971988677978516 - F1 score: 0.5601922869682312\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 106.28813934326172 - F1 score: 0.1438215672969818\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 27.537641525268555 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 19.175622940063477 - F1 score: 0.7096501588821411\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 14.886488914489746 - F1 score: 0.6266946196556091\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.40807172656059265 (+- 0.28221866196780065)\n",
      "> Loss: 39.171976280212405\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.42443394660949707 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.42092499136924744 - F1 score: 0.39836597442626953\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4820605218410492 - F1 score: 0.7089542150497437\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6395629644393921 - F1 score: 0.4904397428035736\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.41489389538764954 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.31955198645591737 (+- 0.2797410623054867)\n",
      "> Loss: 0.4763752639293671\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----binary_crossentropy**************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5289651155471802 - F1 score: 0.68458491563797\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.7361085414886475 - F1 score: 0.4885254502296448\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 2.142195463180542 - F1 score: 0.6689332723617554\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.7080197930335999 - F1 score: 0.5067823529243469\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6473245620727539 - F1 score: 0.7109264731407166\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6119504928588867 (+- 0.09445964218690801)\n",
      "> Loss: 0.9525226950645447\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3571385145187378 - F1 score: 0.14604870975017548\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.41300126910209656 - F1 score: 0.26434779167175293\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4017191231250763 - F1 score: 0.1890767365694046\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.27647748589515686 - F1 score: 0.5059164762496948\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.44384971261024475 - F1 score: 0.07287448644638062\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.2356528401374817 (+- 0.14870162022290392)\n",
      "> Loss: 0.37843722105026245\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4307442307472229 - F1 score: 0.6203471422195435\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.29843348264694214 - F1 score: 0.6907221078872681\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.41516345739364624 - F1 score: 0.7836896181106567\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5680047869682312 - F1 score: 0.8379727602005005\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.38804858922958374 - F1 score: 0.6339690089225769\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7133401274681092 (+- 0.08478371134816078)\n",
      "> Loss: 0.4200789093971252\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.29776445031166077 - F1 score: 0.3318295478820801\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.16214604675769806 - F1 score: 0.7984437942504883\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.22805629670619965 - F1 score: 0.7057815790176392\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.42516785860061646 - F1 score: 0.313565194606781\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.1253669410943985 - F1 score: 0.6430929899215698\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5585426211357116 (+- 0.19889364270115362)\n",
      "> Loss: 0.24770031869411469\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.0496197938919067 - F1 score: 0.7727318406105042\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.8607258200645447 - F1 score: 0.5325435996055603\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.8905410170555115 - F1 score: 0.8220506906509399\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.49012765288352966 - F1 score: 0.6093960404396057\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.44017642736434937 - F1 score: 0.870037853717804\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7213520050048828 (+- 0.12889557506474944)\n",
      "> Loss: 0.7462381422519684\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.22763311862945557 - F1 score: 0.5615338087081909\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1722930520772934 - F1 score: 0.797664999961853\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.32131102681159973 - F1 score: 0.38786545395851135\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.40608805418014526 - F1 score: 0.5776125192642212\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.30681806802749634 - F1 score: 0.3974509537220001\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5444255471229553 (+- 0.1494234801398721)\n",
      "> Loss: 0.28682866394519807\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.601247251033783 - F1 score: 0.7159008979797363\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4974503815174103 - F1 score: 0.7751691341400146\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.47995197772979736 - F1 score: 0.5868695378303528\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5277920365333557 - F1 score: 0.5768784284591675\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6451380848884583 - F1 score: 0.544725239276886\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6399086475372314 (+- 0.08934882007829843)\n",
      "> Loss: 0.5503159463405609\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.24914847314357758 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2491769641637802 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.24948544800281525 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2135503590106964 - F1 score: 0.44955071806907654\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2336551398038864 - F1 score: 0.448363721370697\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.1795828878879547 (+- 0.21994354123134136)\n",
      "> Loss: 0.23900327682495118\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for opti in optimzers:\n",
    "    for los in lossFunctions:\n",
    "        evaluateModel(opti,los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************adam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.14420747756958 - F1 score: 0.7012826204299927\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.0509024858474731 - F1 score: 0.4623781144618988\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5976658463478088 - F1 score: 0.8463636636734009\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.38333842158317566 - F1 score: 0.6534032225608826\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.4168491065502167 - F1 score: 0.6223899722099304\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6571635186672211 (+- 0.12402248430197545)\n",
      "> Loss: 0.7185926675796509\n",
      "------------------------------------------------------------------------\n",
      "**************************************adam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.292063444852829 - F1 score: 0.5175867080688477\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.16300718486309052 - F1 score: 0.8456709980964661\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.15968038141727448 - F1 score: 0.7666369080543518\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.13942602276802063 - F1 score: 0.8229669332504272\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.16683432459831238 - F1 score: 0.8503341674804688\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7606391429901123 (+- 0.12511660684881828)\n",
      "> Loss: 0.18420227169990538\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6670301556587219 - F1 score: 0.6926524043083191\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6896699070930481 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.681708037853241 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6904590129852295 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.687516987323761 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.1385304808616638 (+- 0.2770609617233276)\n",
      "> Loss: 0.6832768201828003\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.20575185120105743 - F1 score: 0.5563029646873474\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1929326057434082 - F1 score: 0.5219393968582153\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5558932423591614 - F1 score: 0.6085031032562256\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2397850751876831 - F1 score: 0.08214285224676132\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2537292242050171 - F1 score: 0.09309461712837219\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.37239658683538435 (+- 0.2341741726430983)\n",
      "> Loss: 0.28961839973926545\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.49824488162994385 - F1 score: 0.8031526803970337\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.43492114543914795 - F1 score: 0.8309383392333984\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6572539210319519 - F1 score: 0.8282841444015503\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4866344928741455 - F1 score: 0.6646279692649841\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.7801200747489929 - F1 score: 0.9044200778007507\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.8062846422195434 (+- 0.07848409872940251)\n",
      "> Loss: 0.5714349031448365\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.20928464829921722 - F1 score: 0.5620601773262024\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.16681137681007385 - F1 score: 0.77558434009552\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.12771891057491302 - F1 score: 0.786202609539032\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.14722932875156403 - F1 score: 0.6363445520401001\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.1295739710330963 - F1 score: 0.6167032122612\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6753789782524109 (+- 0.08958869837361712)\n",
      "> Loss: 0.15612364709377288\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 2.7729029655456543 - F1 score: 0.6013703942298889\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 9.654264450073242 - F1 score: 0.2940669655799866\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 73.6860580444336 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 16.238513946533203 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 26.109811782836914 - F1 score: 0.6319318413734436\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.3054738402366638 (+- 0.2760029884253394)\n",
      "> Loss: 25.69231023788452\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4220821261405945 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4016881585121155 - F1 score: 0.33560243248939514\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4405316114425659 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4238108694553375 - F1 score: 0.026666665449738503\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.4593795835971832 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.07245381958782673 (+- 0.13197903157295904)\n",
      "> Loss: 0.4294984698295593\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----binary_crossentropy**************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5449233651161194 - F1 score: 0.764435887336731\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6451534032821655 - F1 score: 0.5776469707489014\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5919827222824097 - F1 score: 0.4603589177131653\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6074787974357605 - F1 score: 0.45157891511917114\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5853383541107178 - F1 score: 0.7357404828071594\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5979522347450257 (+- 0.13226623244077274)\n",
      "> Loss: 0.5949753284454345\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.505843997001648 - F1 score: 0.6668342351913452\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.3369092643260956 - F1 score: 0.2541176378726959\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.42919066548347473 - F1 score: 0.10794313251972198\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24207404255867004 - F1 score: 0.712429404258728\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3640715181827545 - F1 score: 0.3293333053588867\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.41413154304027555 (+- 0.23638392429514024)\n",
      "> Loss: 0.37561789751052854\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.36411261558532715 - F1 score: 0.8808576464653015\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5518322587013245 - F1 score: 0.8065627813339233\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.42781707644462585 - F1 score: 0.7448593974113464\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.37880054116249084 - F1 score: 0.6417508125305176\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.4181036949157715 - F1 score: 0.844761848449707\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7837584972381592 (+- 0.08408211640551862)\n",
      "> Loss: 0.42813323736190795\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.1568429321050644 - F1 score: 0.6454545259475708\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.15943863987922668 - F1 score: 0.6057777404785156\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.15640844404697418 - F1 score: 0.8252236247062683\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2149449735879898 - F1 score: 0.8064661026000977\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.16560176014900208 - F1 score: 0.6065627336502075\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.697896945476532 (+- 0.09754736148932754)\n",
      "> Loss: 0.17064734995365144\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5256890058517456 - F1 score: 0.5669773817062378\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.433891624212265 - F1 score: 0.8136268854141235\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4276587665081024 - F1 score: 0.8396280407905579\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.1131787300109863 - F1 score: 0.8007717132568359\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.1397379636764526 - F1 score: 0.8198699951171875\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7681748032569885 (+- 0.10137732563854608)\n",
      "> Loss: 0.7280312180519104\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4041396379470825 - F1 score: 0.1249999850988388\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1429262012243271 - F1 score: 0.6473805904388428\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.209627166390419 - F1 score: 0.7574210166931152\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3546655774116516 - F1 score: 0.18409088253974915\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.14988042414188385 - F1 score: 0.8454629778862\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5118710905313492 (+- 0.2990151350957969)\n",
      "> Loss: 0.2522478014230728\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5999763011932373 - F1 score: 0.4760518968105316\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6364429593086243 - F1 score: 0.6918756365776062\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.563332736492157 - F1 score: 0.5165567398071289\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6388245820999146 - F1 score: 0.7019356489181519\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5533971786499023 - F1 score: 0.5477241277694702\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5868288099765777 (+- 0.09276129004825727)\n",
      "> Loss: 0.5983947515487671\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.24457211792469025 - F1 score: 0.35888195037841797\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.22544685006141663 - F1 score: 0.5311864614486694\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2495957612991333 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24910560250282288 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.23405885696411133 - F1 score: 0.3999219536781311\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.2579980731010437 (+- 0.21820992257376923)\n",
      "> Loss: 0.24055583775043488\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_model(optimizerF, lossF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model\n",
    "\n",
    "\n",
    "for opti in optimzers:\n",
    "    for los in lossFunctions:\n",
    "        evaluateModel(opti,los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************adam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.618331789970398 - F1 score: 0.6437663435935974\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.587186872959137 - F1 score: 0.6531841158866882\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6479395031929016 - F1 score: 0.7877402305603027\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.47950997948646545 - F1 score: 0.8073049783706665\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6319595575332642 - F1 score: 0.6771808862686157\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7138353109359741 (+- 0.06946983571026938)\n",
      "> Loss: 0.5929855406284332\n",
      "------------------------------------------------------------------------\n",
      "**************************************adam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.18302783370018005 - F1 score: 0.7744867205619812\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.20151621103286743 - F1 score: 0.6122421026229858\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.36266273260116577 - F1 score: 0.5409780740737915\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.16336175799369812 - F1 score: 0.8496195077896118\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.14928895235061646 - F1 score: 0.6250182390213013\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6804689288139343 (+- 0.11370016789758083)\n",
      "> Loss: 0.21197149753570557\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6575470566749573 - F1 score: 0.7011288404464722\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6701782941818237 - F1 score: 0.6410255432128906\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6671820878982544 - F1 score: 0.14855992794036865\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6560495495796204 - F1 score: 0.21547618508338928\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.66786789894104 - F1 score: 0.5457352995872498\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.4503851592540741 (+- 0.22564950490481095)\n",
      "> Loss: 0.6637649774551392\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.443400502204895 - F1 score: 0.15238094329833984\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2290487289428711 - F1 score: 0.7246842384338379\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.34297975897789 - F1 score: 0.46887579560279846\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3876670002937317 - F1 score: 0.10229691118001938\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2130226343870163 - F1 score: 0.49777770042419434\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.389203117787838 (+- 0.23198434068479018)\n",
      "> Loss: 0.3232237249612808\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.2190617322921753 - F1 score: 0.7629795074462891\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6191850304603577 - F1 score: 0.8841754198074341\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.081164836883545 - F1 score: 0.7498874068260193\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5635875463485718 - F1 score: 0.8428329229354858\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6166913509368896 - F1 score: 0.6044756174087524\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7688701748847961 (+- 0.09611077315230376)\n",
      "> Loss: 0.8199380993843078\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.17380894720554352 - F1 score: 0.7428675293922424\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.15088440477848053 - F1 score: 0.7908898591995239\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3218866288661957 - F1 score: 0.4727397561073303\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.12626294791698456 - F1 score: 0.8335564732551575\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.19865268468856812 - F1 score: 0.6304160952568054\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6940939426422119 (+- 0.12980805353733482)\n",
      "> Loss: 0.19429912269115449\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 30.686506271362305 - F1 score: 0.37272605299949646\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 16.65507698059082 - F1 score: 0.6428921222686768\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 130.1393585205078 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 108.532470703125 - F1 score: 0.11682538688182831\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.107343673706055 - F1 score: 0.6498299837112427\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.35645470917224886 (+- 0.26565480298861033)\n",
      "> Loss: 58.0241512298584\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.41609102487564087 - F1 score: 0.059999991208314896\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5190503001213074 - F1 score: 0.52765953540802\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4649410545825958 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.31505686044692993 - F1 score: 0.6481121778488159\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.29753056168556213 - F1 score: 0.7314563989639282\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.3934456206858158 (+- 0.3043370370917689)\n",
      "> Loss: 0.40253396034240724\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----binary_crossentropy**************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.575583279132843 - F1 score: 0.6447864770889282\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5522504448890686 - F1 score: 0.6772712469100952\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6336426138877869 - F1 score: 0.5185964107513428\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5705224871635437 - F1 score: 0.7348781824111938\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5348223447799683 - F1 score: 0.5123993754386902\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6175863385200501 (+- 0.08823017111284773)\n",
      "> Loss: 0.5733642339706421\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.22413960099220276 - F1 score: 0.6124874353408813\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.37805813550949097 - F1 score: 0.1607142835855484\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2768520414829254 - F1 score: 0.5321781039237976\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.39633724093437195 - F1 score: 0.3482683300971985\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.47442784905433655 - F1 score: 0.5411111116409302\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.4389518529176712 (+- 0.16424487370112034)\n",
      "> Loss: 0.34996297359466555\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.40063580870628357 - F1 score: 0.670983612537384\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4062070846557617 - F1 score: 0.8549787402153015\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5868820548057556 - F1 score: 0.815966010093689\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.40733423829078674 - F1 score: 0.6006776094436646\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.327623575925827 - F1 score: 0.8704283833503723\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7626068711280822 (+- 0.10735096246625582)\n",
      "> Loss: 0.4257365524768829\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.166097953915596 - F1 score: 0.7655612230300903\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.3211957812309265 - F1 score: 0.3966006934642792\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.120200514793396 - F1 score: 0.6411482095718384\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.31868037581443787 - F1 score: 0.3628700375556946\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.12220975011587143 - F1 score: 0.8340546488761902\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6000469624996185 (+- 0.1905188485916351)\n",
      "> Loss: 0.20967687517404557\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5634907484054565 - F1 score: 0.7540449500083923\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5002568960189819 - F1 score: 0.6606758236885071\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.830978274345398 - F1 score: 0.682301938533783\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.5876185894012451 - F1 score: 0.8212478756904602\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.4302719235420227 - F1 score: 0.6587913632392883\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7154123902320861 (+- 0.06323283811983145)\n",
      "> Loss: 0.7825232863426208\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.19204165041446686 - F1 score: 0.8295074701309204\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.23485130071640015 - F1 score: 0.7090321779251099\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.17985327541828156 - F1 score: 0.7208994626998901\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.11724383383989334 - F1 score: 0.6587932705879211\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.1917646825313568 - F1 score: 0.6042208075523376\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7044906377792358 (+- 0.07491511424989127)\n",
      "> Loss: 0.18315094858407974\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6090312004089355 - F1 score: 0.6636781096458435\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6177586913108826 - F1 score: 0.6687878370285034\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.497306764125824 - F1 score: 0.8200503587722778\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5842364430427551 - F1 score: 0.45496225357055664\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5867899656295776 - F1 score: 0.5166438817977905\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6248244881629944 (+- 0.12814971153711988)\n",
      "> Loss: 0.579024612903595\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.24969106912612915 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.24914535880088806 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2454974353313446 - F1 score: 0.29859647154808044\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.25006240606307983 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2271278351545334 - F1 score: 0.48431292176246643\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.15658187866210938 (+- 0.20056392266820214)\n",
      "> Loss: 0.244304820895195\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_model(optimizerF, lossF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model\n",
    "\n",
    "\n",
    "for opti in optimzers:\n",
    "    for los in lossFunctions:\n",
    "        evaluateModel(opti,los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************adam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.4515506029129028 - F1 score: 0.8064724802970886\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.3350119590759277 - F1 score: 0.5176111459732056\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.073710322380066 - F1 score: 0.8655301928520203\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.098099708557129 - F1 score: 0.8816707730293274\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.8251869082450867 - F1 score: 0.8820139169692993\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7906597018241882 (+- 0.13931242295558216)\n",
      "> Loss: 1.1567119002342223\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2,l1\n",
    "\n",
    "def get_model(optimizerF, lossF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu', kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model\n",
    "evaluateModel('adam','binary_crossentropy')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitb677fe8f554d432f967ed92773df8dd7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
